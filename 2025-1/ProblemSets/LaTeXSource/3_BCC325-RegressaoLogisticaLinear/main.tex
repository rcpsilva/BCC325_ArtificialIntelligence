% exercise_list_regression.tex -----------------------------------------
\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[brazil]{babel}
\usepackage{lmodern}
\usepackage{amsmath, amssymb, amsfonts, bm}
\usepackage{booktabs}
\usepackage{geometry}
\geometry{a4paper, margin=2.5cm}

\title{Lista de Exercícios \textemdash{} Regressão Logística e Regressão Linear}
\author{}
\date{}

\begin{document}
\maketitle

Use apenas papel, caneta e calculadora básica. Nada de Python nesta parte!

--------------------------------------------------------------------------
\section*{Parte A \;\textemdash\; Regressão Logística}

Considere sempre a função sigmoide
\[
\sigma(z)=\frac{1}{1+e^{-z}}.
\]

\subsection*{Conjunto\,\scriptsize{\,(3~pontos)}}

\begin{center}
\begin{tabular}{cccc}
\toprule
\(i\) & \(x_{1i}\) & \(x_{2i}\) & \(y_i\)\\
\midrule
1 & 0 & 0 & 0\\
2 & 1 & 0 & 0\\
3 & 0 & 1 & 1\\
\bottomrule
\end{tabular}
\end{center}

\begin{enumerate}
  \item \textbf{Predições iniciais}. Tome \(w_0=w_1=w_2=1\). Calcule \(z_i\) e \(\hat y_i=\sigma(z_i)\) para cada linha.
  \item \textbf{Gradientes}. Derive \(\partial J/\partial w_0,\partial J/\partial w_1,\partial J/\partial w_2\) para entropia-cruzada ($-log(verossimilhança)$) e substitua os valores do item\,1.
  \item \textbf{Primeira atualização}. No algoritmo de descida do gradiente, use taxa \(\alpha=0.4\) e calcule a primeira atualização dos pesos. Mantenha quatro casas decimais.
  \item \textbf{Perda}. Com os novos pesos, recalcule \(\hat y_i\) e avalie \(J\).
  \item \textbf{Segunda iteração}. Repita os itens\,2--4. Compare o novo \(J\) com o anterior.
  \item \textbf{Variação da taxa}. Refaça apenas a primeira atualização com \(\alpha=1.2\) e discuta.
\end{enumerate}

--------------------------------------------------------------------------
\section*{Parte B \;\textemdash\; Regressão Linear por Mínimos Quadrados}

\begin{center}
\begin{tabular}{cccc}
\toprule
\(i\) & \(x_{1i}\) & \(x_{2i}\) & \(y_i\)\\
\midrule
1 & 0 & 0 & 1\\
2 & 1 & 0 & 2\\
3 & 0 & 1 & 2\\
4 & 1 & 1 & 3\\
\bottomrule
\end{tabular}
\end{center}

\begin{enumerate}
  \item Monte \(X\) (com coluna de 1’s) e \(\bm y\).
  \item Calcule \(X^{\!\top}X\) e \(X^{\!\top}\bm y\).
  \item Inverta \(X^{\!\top}X\) via \textbf{Gauss--Jordan}. Documente cada operação de linha.
  \item Inverta novamente pelo \textbf{método da adjunta}: cofatores, adjunta e determinante.
  \item Obtenha os pesos \(\hat{\bm w}\) multiplicando a inversa por \(X^{\!\top}\bm y\).
  \item Calcule o erro quadrático do modelo com os pesos calculados.
\end{enumerate}

\subsection*{Novo conjunto}

Dados \((x_1,x_2,y)\in\{(0,0,2), (2,0,3), (0,2,3), (2,2,6)\}\), repita os itens~1--6 acima e verifique se os dois métodos de inversão produzem o mesmo \(\hat{\bm w}\).

--------------------------------------------------------------------------
\section*{Parte C \;\textemdash\; Regressão linear com descida do gradiente}

\begin{enumerate}
  \item Derive o gradiente de \(J(\bm w)=\tfrac12\lVert X\bm w-\bm y\rVert^{2}\) e execute três iterações manuais de descida do gradiente com \(\alpha=0.1\) no primeiro conjunto da Parte~B.
  \item Para cada iteração, calcule o erro quadrado total e compare com o erro quadrado do método analítico. Comente a taxa de convergência.
\end{enumerate}


\end{document}
% ----------------------------------------------------------------------
